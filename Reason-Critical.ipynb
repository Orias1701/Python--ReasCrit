{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ef9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_cpp import Llama\n",
    "from statistics import mean\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from Libraries import Common_Utils as UTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ff5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORIES = Path(\"Data/Histories.json\")\n",
    "CONFIG_PATH = Path(\"Config/config.json\")\n",
    "\n",
    "# T·ª± ƒë·ªông t·∫°o th∆∞ m·ª•c Data ƒë·ªÉ l∆∞u l·ªãch s·ª≠\n",
    "HISTORIES.parent.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Th∆∞ m·ª•c l∆∞u l·ªãch s·ª≠: {HISTORIES.parent.resolve()}\")\n",
    "print(f\"ƒê∆∞·ªùng d·∫´n file config: {CONFIG_PATH.resolve()}\")\n",
    "\n",
    "# --- 2. Khai b√°o c√°c bi·∫øn to√†n c·ª•c ---\n",
    "REASON_CLIENT = None\n",
    "CRITIC_CLIENT = None\n",
    "HF_DATASET = None\n",
    "REASON_PROMPT = \"\"\n",
    "CRITIC_PROMPT = \"\"\n",
    "CONFIG = {}\n",
    "GENERATION_PARAMS = {}\n",
    "LLAMA_CPP_PARAMS = {}\n",
    "FLOW_PARAMS = {}\n",
    "\n",
    "# --- 3. T·∫£i file c·∫•u h√¨nh ch√≠nh ---\n",
    "print(f\"ƒêang t·∫£i config t·ª´: {CONFIG_PATH}\")\n",
    "CONFIG = UTL.read_json(CONFIG_PATH)\n",
    "\n",
    "# --- 4. T·∫£i c√°c file Prompt v√† l·∫•y C·∫•u h√¨nh d·ª±a tr√™n config ---\n",
    "if CONFIG:\n",
    "    PROMPT_REASON_PATH = Path(CONFIG[\"paths\"][\"prompt_reason\"])\n",
    "    PROMPT_CRITIC_PATH = Path(CONFIG[\"paths\"][\"prompt_critic\"])\n",
    "\n",
    "    print(f\"ƒêang t·∫£i Reason prompt t·ª´: {PROMPT_REASON_PATH}\")\n",
    "    REASON_PROMPT = UTL.read_text(PROMPT_REASON_PATH)\n",
    "    \n",
    "    print(f\"ƒêang t·∫£i Critic prompt t·ª´: {PROMPT_CRITIC_PATH}\")\n",
    "    CRITIC_PROMPT = UTL.read_text(PROMPT_CRITIC_PATH)\n",
    "    \n",
    "    GENERATION_PARAMS = CONFIG.get(\"generation_params\", {})\n",
    "    LLAMA_CPP_PARAMS = CONFIG.get(\"llama_cpp_params\", {})\n",
    "    FLOW_PARAMS = CONFIG.get(\"flow_params\", {})\n",
    "\n",
    "# --- 5. Ki·ªÉm tra k·∫øt qu·∫£ t·∫£i file ---\n",
    "match (bool(CONFIG), bool(REASON_PROMPT), bool(CRITIC_PROMPT)):\n",
    "    case (False, _, _):\n",
    "        print(\"‚õî D·ª´ng ch∆∞∆°ng tr√¨nh do kh√¥ng t·∫£i ƒë∆∞·ª£c file config.\")\n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file config t·∫°i {CONFIG_PATH.resolve()}\")\n",
    "    case (_, False, _) | (_, _, False):\n",
    "        print(\"‚õî D·ª´ng ch∆∞∆°ng tr√¨nh do kh√¥ng t·∫£i ƒë∆∞·ª£c file prompt.\")\n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file prompt (Reason: {PROMPT_REASON_PATH.resolve()} ho·∫∑c Critic: {PROMPT_CRITIC_PATH.resolve()})\")\n",
    "    case _:\n",
    "        print(\"‚úÖ T·∫•t c·∫£ file c·∫•u h√¨nh v√† prompt ƒë√£ t·∫£i th√†nh c√¥ng.\")\n",
    "        print(f\"C·∫•u h√¨nh Llama.cpp s·∫Ω d√πng: {LLAMA_CPP_PARAMS}\")\n",
    "        print(f\"C·∫•u h√¨nh Flow s·∫Ω d√πng: {FLOW_PARAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbec5d",
   "metadata": {},
   "source": [
    "### HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset():\n",
    "    \"\"\"T·∫£i dataset t·ª´ Hugging Face.\"\"\"\n",
    "    global HF_DATASET\n",
    "    DATASET_NAME = CONFIG[\"dataset\"][\"name\"]\n",
    "    DATASET_SPLIT = CONFIG[\"dataset\"].get(\"split\", \"train\")\n",
    "    \n",
    "    print(f\"üìò ƒêang t·∫£i dataset '{DATASET_NAME}' (split: {DATASET_SPLIT})...\")\n",
    "    try:\n",
    "        HF_DATASET = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "        print(f\"‚úÖ T·∫£i dataset th√†nh c√¥ng. (S·ªë l∆∞·ª£ng: {len(HF_DATASET)} m·∫´u)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t·∫£i dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonParse(text: str):\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        match = regex.search(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=regex.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(0)\n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        raise ValueError(f\"‚ùå Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ ph·∫£n h·ªìi:\\n{text}\")\n",
    "    \n",
    "def randomContent() -> str:\n",
    "    if HF_DATASET is None:\n",
    "        print(\"‚ùå L·ªói: Dataset ch∆∞a ƒë∆∞·ª£c t·∫£i. (HF_DATASET is None)\")\n",
    "        return None\n",
    "    try:\n",
    "        idx = random.randint(0, len(HF_DATASET) - 1)\n",
    "        sample = HF_DATASET[idx]\n",
    "        content = sample.get(\"article\") or sample.get(\"text\")\n",
    "        title = sample.get(\"headlines\", \"Kh√¥ng c√≥ ti√™u ƒë·ªÅ\")\n",
    "        if not content:\n",
    "            print(f\"‚ö†Ô∏è M·∫´u ng·∫´u nhi√™n (index {idx}) kh√¥ng c√≥ c·ªôt 'article' ho·∫∑c 'text', th·ª≠ l·∫°i...\")\n",
    "            return randomContent()\n",
    "        print(f\"üìò Ch·ªçn ng·∫´u nhi√™n m·ª•c: {title}\")\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi l·∫•y m·∫´u ng·∫´u nhi√™n t·ª´ dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def averageScore(critical_output: dict) -> float:\n",
    "    \"\"\"T√≠nh ƒëi·ªÉm trung b√¨nh t·ª´ output JSON l·ªìng nhau c·ªßa critical model.\"\"\"\n",
    "    scoring_dict = critical_output.get(\"scoring\")\n",
    "    if not scoring_dict or not isinstance(scoring_dict, dict):\n",
    "        if \"error\" not in critical_output:\n",
    "             print(\"‚ö†Ô∏è L·ªói: Kh√¥ng t√¨m th·∫•y key 'scoring' trong output c·ªßa Critical.\")\n",
    "        return 0.0\n",
    "    keys = [\"factuality\", \"clarity\", \"logical_coherence\", \"coverage\", \"utility\", \"consistency\"]\n",
    "    vals = [scoring_dict[k] for k in keys if isinstance(scoring_dict.get(k), (int, float))]\n",
    "    return mean(vals) if vals else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm_clients():\n",
    "    \"\"\"\n",
    "    T·∫£i v·ªÅ v√† kh·ªüi t·∫°o c·∫£ hai m√¥ h√¨nh LLM (Reasoning v√† Critical) t·ª´ Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    global REASON_CLIENT, CRITIC_CLIENT\n",
    "    \n",
    "    try:\n",
    "        # L·∫•y th√¥ng s·ªë chung cho LlamaCpp\n",
    "        llama_params = CONFIG.get(\"llama_cpp_params\", {})\n",
    "        print(f\"S·ª≠ d·ª•ng Llama params: {llama_params}\")\n",
    "        \n",
    "        # --- 1. Kh·ªüi t·∫°o REASONING MODEL (Qwen 1.8B) ---\n",
    "        print(\"\\n--- 1. ƒêang t·∫£i Reasoning Model (Qwen 1.8B) ---\")\n",
    "        reason_config = CONFIG[\"models\"][\"reasoning_model\"]\n",
    "        \n",
    "        print(f\"Downloading {reason_config['hf_filename']} t·ª´ {reason_config['hf_repo_id']}...\")\n",
    "        reason_model_path = hf_hub_download(\n",
    "            repo_id=reason_config[\"hf_repo_id\"],\n",
    "            filename=reason_config[\"hf_filename\"]\n",
    "        )\n",
    "        print(f\"ƒê√£ t·∫£i xong model Reasoning, ƒë∆∞·ªùng d·∫´n: {reason_model_path}\")\n",
    "        \n",
    "        REASON_CLIENT = Llama(\n",
    "            model_path=reason_model_path,\n",
    "            **llama_params\n",
    "        )\n",
    "        print(\"‚úÖ Reasoning Model (Qwen) ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.\")\n",
    "\n",
    "        # --- 2. Kh·ªüi t·∫°o CRITICAL MODEL (Phi-3 Mini) ---\n",
    "        print(\"\\n--- 2. ƒêang t·∫£i Critical Model (Phi-3 Mini) ---\")\n",
    "        critic_config = CONFIG[\"models\"][\"critical_model\"]\n",
    "        \n",
    "        print(f\"Downloading {critic_config['hf_filename']} t·ª´ {critic_config['hf_repo_id']}...\")\n",
    "        critic_model_path = hf_hub_download(\n",
    "            repo_id=critic_config[\"hf_repo_id\"],\n",
    "            filename=critic_config[\"hf_filename\"]\n",
    "        )\n",
    "        print(f\"ƒê√£ t·∫£i xong model Critical, ƒë∆∞·ªùng d·∫´n: {critic_model_path}\")\n",
    "        \n",
    "        CRITIC_CLIENT = Llama(\n",
    "            model_path=critic_model_path,\n",
    "            **llama_params\n",
    "        )\n",
    "        print(\"‚úÖ Critical Model (Phi-3) ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.\")\n",
    "        print(\"\\n--- T·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë√£ s·∫µn s√†ng! ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ƒê√£ x·∫£y ra l·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o LLM: {e}\")\n",
    "        REASON_CLIENT = None\n",
    "        CRITIC_CLIENT = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb37d5",
   "metadata": {},
   "source": [
    "### RUNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoningRun(text: str, feedback: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Ch·∫°y reasoning (v·ªõi model Phi-2). \n",
    "    S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng prompt 'Instruct/Output' ƒë·∫∑c th√π c·ªßa Phi-2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Ki·ªÉm tra xem REASON_CLIENT (Phi-2) ƒë√£ ƒë∆∞·ª£c t·∫£i ch∆∞a\n",
    "    if REASON_CLIENT is None:\n",
    "        print(\"‚ùå L·ªói: reasoningRun kh√¥ng th·ªÉ ch·∫°y v√¨ REASON_CLIENT (Phi-2) ch∆∞a ƒë∆∞·ª£c t·∫£i.\")\n",
    "        return \"\"\n",
    "        \n",
    "    # 2. X√¢y d·ª±ng prompt\n",
    "    if feedback:\n",
    "        # Ch·∫ø ƒë·ªô refine: Y√™u c·∫ßu Phi-2 l√†m l·∫°i d·ª±a tr√™n feedback\n",
    "        prompt_content = (\n",
    "            f\"B·∫°n ƒë√£ t·∫°o ra m·ªôt b·∫£n t√≥m t·∫Øt tr∆∞·ªõc ƒë√≥. \"\n",
    "            f\"B·∫£n t√≥m t·∫Øt ƒë√≥ ƒë√£ ƒë∆∞·ª£c ƒë√°nh gi√° v·ªõi ph·∫£n h·ªìi sau:\\n\"\n",
    "            f\"---BEGIN FEEDBACK---\\n{feedback}\\n---END FEEDBACK---\\n\\n\"\n",
    "            f\"Vui l√≤ng t·∫°o l·∫°i reasoning trace v√† t√≥m t·∫Øt, c√≥ t√≠nh ƒë·∫øn ph·∫£n h·ªìi n√†y. \"\n",
    "            f\"S·ª≠ d·ª•ng l·∫°i vƒÉn b·∫£n g·ªëc d∆∞·ªõi ƒë√¢y.\\n\\n\"\n",
    "            f\"VƒÉn b·∫£n g·ªëc:\\n{text}\"\n",
    "        )\n",
    "    else:\n",
    "        # L·∫ßn ch·∫°y ƒë·∫ßu ti√™n: S·ª≠ d·ª•ng REASON_PROMPT\n",
    "        prompt_content = f\"{REASON_PROMPT}\\n\\n{text}\"\n",
    "    \n",
    "    # --- THAY ƒê·ªîI QUAN TR·ªåNG ---\n",
    "    # √Åp d·ª•ng ƒë·ªãnh d·∫°ng 'Instruct/Output' c·ªßa Phi-2\n",
    "    prompt = f\"Instruct: {prompt_content}\\nOutput:\"\n",
    "    # --- K·∫æT TH√öC THAY ƒê·ªîI ---\n",
    "    \n",
    "    # print(f\"--- PROMPT (REASONING - Phi-2) ---\\n{prompt}\\n---------------------------\")\n",
    "\n",
    "    # 3. G·ªçi m√¥ h√¨nh REASON_CLIENT (Phi-2)\n",
    "    try:\n",
    "        completion = REASON_CLIENT(\n",
    "            prompt,\n",
    "            max_tokens=GENERATION_PARAMS.get(\"max_new_tokens\", 512),\n",
    "            temperature=GENERATION_PARAMS.get(\"temperature\", 0.3),\n",
    "            top_p=GENERATION_PARAMS.get(\"top_p\", 0.9),\n",
    "            # Phi-2 kh√¥ng d√πng stop token ƒë·∫∑c bi·ªát,\n",
    "            # nh∆∞ng ch√∫ng ta v·∫´n gi·ªØ c√°c token chung\n",
    "            stop=[\"<|end|>\", \"<|endoftext|>\", \"</s>\", \"\\nInstruct:\"] \n",
    "        )\n",
    "        \n",
    "        if \"choices\" in completion and len(completion[\"choices\"]) > 0:\n",
    "            return completion[\"choices\"][0][\"text\"].strip()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è C·∫£nh b√°o: Response t·ª´ REASON_CLIENT (Phi-2) c√≥ c·∫•u tr√∫c l·∫°: {completion}\")\n",
    "            return str(completion).strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi ch·∫°y reasoningRun (REASON_CLIENT - Phi-2): {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticalRun(source_text: str, reasoning_output: str) -> dict:\n",
    "    \"\"\"Ch·∫°y critical B·∫∞NG M√î H√åNH LOCAL (Phi-3), tr·∫£ v·ªÅ dict JSON.\"\"\"\n",
    "    \n",
    "    if CRITIC_CLIENT is None:\n",
    "        print(\"‚ùå L·ªói: criticalRun kh√¥ng th·ªÉ ch·∫°y v√¨ CRITIC_CLIENT (Phi-3) ch∆∞a ƒë∆∞·ª£c t·∫£i.\")\n",
    "        return {\"error\": \"CRITIC_CLIENT (Phi-3) not initialized\"}\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    {CRITIC_PROMPT}\n",
    "\n",
    "    [Reasoning trace]:\n",
    "    {reasoning_output}\n",
    "\n",
    "    [VƒÉn b·∫£n g·ªëc]:\n",
    "    {source_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    final_prompt = f\"<|user|>\\n{user_prompt}<|end|>\\n<|assistant|>\"\n",
    "\n",
    "    try:\n",
    "        completion = CRITIC_CLIENT(\n",
    "            final_prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            \n",
    "            stop=[\"<|end|>\", \"```\", \"===\"] \n",
    "        )\n",
    "\n",
    "        if \"choices\" not in completion or len(completion[\"choices\"]) == 0:\n",
    "            raise ValueError(\"M√¥ h√¨nh CRITIC_CLIENT (Phi-3) kh√¥ng tr·∫£ v·ªÅ n·ªôi dung.\")\n",
    "\n",
    "        raw_output = completion[\"choices\"][0][\"text\"].strip()\n",
    "        if not raw_output:\n",
    "            raise ValueError(\"M√¥ h√¨nh CRITIC_CLIENT (Phi-3) tr·∫£ v·ªÅ ph·∫£n h·ªìi r·ªóng.\")\n",
    "\n",
    "        try:\n",
    "            # B√¢y gi·ªù raw_output s·∫Ω s·∫°ch h∆°n v√† ch·ªâ ch·ª©a JSON\n",
    "            parsed_json = jsonParse(raw_output)\n",
    "            return parsed_json\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói parse JSON t·ª´ m√¥ h√¨nh CRITIC_CLIENT (Phi-3): {e}\")\n",
    "            print(f\"--- RAW OUTPUT T·ª™ PHI-3 (CRITICAL) ---\")\n",
    "            print(raw_output)\n",
    "            print(f\"----------------------------------------\")\n",
    "            return {\"error\": \"Failed to parse JSON from critical model\", \"raw_response\": raw_output}\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"‚ùå L·ªói khi g·ªçi CRITIC_CLIENT (critical): {str(e)}\"\n",
    "        print(error_message)\n",
    "        return {\"error\": error_message}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be361b",
   "metadata": {},
   "source": [
    "### MAIN FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainFlow(source_text: str, \n",
    "             max_iters=CONFIG[\"flow_params\"][\"max_iters\"], \n",
    "             min_improve=CONFIG[\"flow_params\"][\"min_improve\"]):\n",
    "    \n",
    "    best_reason, best_score = None, 0\n",
    "    history = []\n",
    "    current_feedback = None\n",
    "    current_reasoning = \"\"\n",
    "\n",
    "    for step in range(1, max_iters + 1):\n",
    "        print(f\"\\nüîÑ V√≤ng {step} ...\")\n",
    "        \n",
    "        current_reasoning = reasoningRun(source_text, current_feedback)\n",
    "        \n",
    "        artifact_marker = \"CV=\" \n",
    "        if artifact_marker in current_reasoning:\n",
    "            print(f\"‚ö†Ô∏è Ph√°t hi·ªán artifact '{artifact_marker}', ƒëang c·∫Øt b·ªè...\")\n",
    "            artifact_index = current_reasoning.find(artifact_marker)\n",
    "            current_reasoning = current_reasoning[:artifact_index].strip()\n",
    "        \n",
    "        if not current_reasoning:\n",
    "            print(\"‚õî Reasoning tr·∫£ v·ªÅ r·ªóng, d·ª´ng v√≤ng l·∫∑p.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nüîÑ Reaoning Result: {current_reasoning}\")\n",
    "        \n",
    "        critical_output = criticalRun(source_text, current_reasoning)\n",
    "        \n",
    "        if critical_output.get(\"error\"):\n",
    "            print(f\"‚õî L·ªói t·ª´ Critical: {critical_output['error']}, d·ª´ng v√≤ng l·∫∑p.\")\n",
    "            break\n",
    "        \n",
    "        average_score = averageScore(critical_output)\n",
    "        \n",
    "        current_feedback = critical_output.get(\"feedback_text\", \"\") \n",
    "\n",
    "        print(f\"üìä ƒêi·ªÉm TBC: {average_score:.2f}\")\n",
    "        print(f\"üìù Nh·∫≠n x√©t (To√†n b·ªô JSON): {json.dumps(critical_output, ensure_ascii=False, indent=2)}\\n\")\n",
    "\n",
    "        history.append({\n",
    "            \"round\": step, \n",
    "            \"score\": average_score, \n",
    "            \"reasoning\": current_reasoning, \n",
    "            \"evaluation\": critical_output.get(\"scoring\", {}),\n",
    "            \"feedback\": current_feedback\n",
    "        })\n",
    "        \n",
    "        if average_score > best_score + min_improve:\n",
    "            best_reason, best_score = current_reasoning, average_score\n",
    "            print(f\"üìà C·∫£i thi·ªán t·ªët, ƒëi·ªÉm m·ªõi: {best_score:.2f}\")\n",
    "            if average_score >= 4.8:\n",
    "                print(\"üéâ ƒê·∫°t ƒëi·ªÉm cao, d·ª´ng s·ªõm.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"‚õî Kh√¥ng c·∫£i thi·ªán ƒë√°ng k·ªÉ, d·ª´ng.\")\n",
    "            break\n",
    "\n",
    "    return {\"best_reasoning\": best_reason, \"best_score\": best_score, \"history\": history}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf1d09",
   "metadata": {},
   "source": [
    "### RUN MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ec45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. G·ªåI C√ÅC H√ÄM KH·ªûI T·∫†O ---\n",
    "    # G·ªçi h√†m m·ªõi ƒë·ªÉ t·∫£i c·∫£ 2 model\n",
    "    initialize_llm_clients() \n",
    "    initialize_dataset()\n",
    "    \n",
    "    # --- 2. KI·ªÇM TRA T√ÄI NGUY√äN ---\n",
    "    # ‚¨ÖÔ∏è Ph·∫£i ki·ªÉm tra c·∫£ REASON_CLIENT v√† CRITIC_CLIENT\n",
    "    if REASON_CLIENT is None or CRITIC_CLIENT is None or HF_DATASET is None: \n",
    "        print(\"‚õî Kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu mainFlow (m·ªôt trong c√°c model ho·∫∑c dataset ch∆∞a ƒë∆∞·ª£c t·∫£i).\")\n",
    "        print(\"Vui l√≤ng ki·ªÉm tra l·∫°i l·ªói ·ªü tr√™n.\")\n",
    "        # S·ª≠ d·ª•ng 'pass' thay v√¨ 'exit()' ƒë·ªÉ tr√°nh l√†m crash kernel Colab\n",
    "        pass \n",
    "    else:\n",
    "        # --- 3. CH·∫†Y QUY TR√åNH CH√çNH ---\n",
    "        try:\n",
    "            inputPara = randomContent()\n",
    "            if inputPara is None:\n",
    "                print(\"‚õî Kh√¥ng t·∫£i ƒë∆∞·ª£c n·ªôi dung test data, d·ª´ng ch∆∞∆°ng tr√¨nh.\")\n",
    "                pass\n",
    "            else:\n",
    "                # S·ª≠ d·ª•ng bi·∫øn FLOW_PARAMS to√†n c·ª•c ƒë√£ t·∫£i ·ªü cell config\n",
    "                print(f\"B·∫Øt ƒë·∫ßu mainFlow v·ªõi c√°c tham s·ªë: {FLOW_PARAMS}\")\n",
    "                result = mainFlow(\n",
    "                    inputPara,\n",
    "                    max_iters=FLOW_PARAMS.get(\"max_iters\", 3),\n",
    "                    min_improve=FLOW_PARAMS.get(\"min_improve\", 0.05)\n",
    "                )\n",
    "\n",
    "                # --- 4. IN K·∫æT QU·∫¢ ---\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"üéØ K·∫æT QU·∫¢ CU·ªêI C√ôNG üéØ\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                final_output = {\n",
    "                    \"best_score\": result[\"best_score\"],\n",
    "                    \"best_reasoning\": result[\"best_reasoning\"]\n",
    "                }\n",
    "                \n",
    "                print(json.dumps(final_output, ensure_ascii=False, indent=2))\n",
    "                \n",
    "                # --- 5. L∆ØU L·ªäCH S·ª¨ ---\n",
    "                # ƒê·∫£m b·∫£o th∆∞ m·ª•c 'Data' ƒë√£ ƒë∆∞·ª£c t·∫°o (t·ª´ cell config)\n",
    "                UTL.insert_json(result[\"history\"], HISTORIES, indent=2)\n",
    "                print(f\"\\nüìù L·ªãch s·ª≠ ch·∫°y ƒë·∫ßy ƒë·ªß ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{HISTORIES}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën trong main: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bruh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
