{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from Libraries import Common_Utils as UTL\n",
    "from Libraries import Common_Helpers as helpers\n",
    "from Libraries import Processor_Datasets as ds_proc\n",
    "from Libraries import Processor_Models as model_proc\n",
    "from Libraries import Flow_Reasoning as flow_reason\n",
    "from Libraries import Flow_Critical as flow_critic\n",
    "from Libraries.exceptions import PipelineAbortSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2281691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0 - 999]\n",
    "INDEX_START = 0\n",
    "INDEX_END = 0\n",
    "\n",
    "HISTORIES = Path(\"Data/Histories.json\")\n",
    "CONFIG_PATH = Path(\"Config/config.json\")\n",
    "BASE_MODELS_DIR = Path(\"Models\")\n",
    "BASE_DATA_DIR = Path(\"Data\")\n",
    "\n",
    "HISTORIES.parent.mkdir(parents=True, exist_ok=True)\n",
    "BATCH_HISTORY_FILE = BASE_DATA_DIR / \"Histories-Batch.json\"\n",
    "\n",
    "REASON_CLIENT = None\n",
    "CRITIC_CLIENT = None\n",
    "HF_DATASET = None\n",
    "FIRST_REASON_PATH = \"\"\n",
    "REFINE_REASON_PATH = \"\"\n",
    "FIRST_CRITIC_PATH = \"\"\n",
    "REFINE_CRITIC_PATH = \"\"\n",
    "FIRST_REASON = \"\"\n",
    "REFINE_REASON = \"\"\n",
    "FIRST_CRITIC = \"\"\n",
    "REFINE_CRITIC = \"\"\n",
    "CONFIG = {}\n",
    "REASON_PARAMS = {}\n",
    "CRITIC_PARAMS = {}\n",
    "LLAMA_CPP_PARAMS = {}\n",
    "FLOW_PARAMS = {}\n",
    "\n",
    "CONFIG = UTL.read_json(CONFIG_PATH)\n",
    "\n",
    "if CONFIG:\n",
    "    FIRST_REASON_PATH = Path(CONFIG[\"paths\"][\"prompt_reason_first-vi\"])\n",
    "    REFINE_REASON_PATH = Path(CONFIG[\"paths\"][\"prompt_reason_refine-vi\"])\n",
    "    FIRST_CRITIC_PATH = Path(CONFIG[\"paths\"][\"prompt_critic_first-vi\"])\n",
    "    REFINE_CRITIC_PATH = Path(CONFIG[\"paths\"][\"prompt_critic_refine-vi\"])\n",
    "\n",
    "    FIRST_REASON = UTL.read_text(FIRST_REASON_PATH)\n",
    "    REFINE_REASON = UTL.read_text(REFINE_REASON_PATH)\n",
    "    FIRST_CRITIC = UTL.read_text(FIRST_CRITIC_PATH)\n",
    "    REFINE_CRITIC = UTL.read_text(REFINE_CRITIC_PATH)\n",
    "\n",
    "    REASON_PARAMS = CONFIG.get(\"reason_params\", {})\n",
    "    CRITIC_PARAMS = CONFIG.get(\"critic_params\", {})\n",
    "    LLAMA_CPP_PARAMS = CONFIG.get(\"llama_cpp_params\", {})\n",
    "    FLOW_PARAMS = CONFIG.get(\"flow_params\", {})\n",
    "\n",
    "    print(\"‚úÖ T·∫•t c·∫£ file c·∫•u h√¨nh v√† prompt ƒë√£ t·∫£i th√†nh c√¥ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG:\n",
    "    dataset_config = CONFIG[\"dataset-VI\"]\n",
    "    parts = dataset_config['name'].split('/')\n",
    "    publisher = parts[0] if len(parts) > 1 else 'default'\n",
    "    dataset_name = parts[-1]\n",
    "    local_path = BASE_DATA_DIR / publisher / dataset_name\n",
    "    \n",
    "    if local_path.exists():\n",
    "        print(f\"‚úÖ ƒêang t·∫£i dataset c·ª•c b·ªô t·ª´: {local_path.resolve()}\")\n",
    "        HF_DATASET = ds_proc.load_from_disk_internal(local_path)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y dataset c·ª•c b·ªô. ƒêang t·∫£i v·ªÅ t·ª´ '{dataset_config['name']}'...\")\n",
    "        HF_DATASET = ds_proc.download_and_save_internal(dataset_config, local_path)\n",
    "        \n",
    "    if not HF_DATASET:\n",
    "            print(\"‚ùå T·∫£i dataset th·∫•t b·∫°i.\")\n",
    "else:\n",
    "    print(\"‚õî Config kh√¥ng ƒë∆∞·ª£c t·∫£i. B·ªè qua Giai ƒëo·∫°n 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb262640",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG: \n",
    "    REASON_CLIENT, CRITIC_CLIENT = model_proc.llm_initialize(\n",
    "        config=CONFIG,\n",
    "        llama_cpp_params=LLAMA_CPP_PARAMS,\n",
    "        base_models_dir=BASE_MODELS_DIR\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f014699",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_DATASET:\n",
    "    print(\"\\n--- üìä Ph√¢n t√≠ch Dataset ---\")\n",
    "    analysis = ds_proc.analyze_dataset_internal(HF_DATASET)\n",
    "    if \"error\" in analysis:\n",
    "        print(f\"L·ªói ph√¢n t√≠ch: {analysis['error']}\")\n",
    "    else:\n",
    "        print(f\"S·ªë l∆∞·ª£ng m·∫´u t·ªïng: {analysis['count']}\")\n",
    "        print(f\"C·∫•u tr√∫c (Features): {analysis['features']}\")\n",
    "        print(f\"T√¨m th·∫•y 'article': {analysis['has_article']}\")\n",
    "        print(f\"T√¨m th·∫•y 'summary': {analysis['has_summary']}\")\n",
    "    print(\".\"*50)\n",
    "\n",
    "    if INDEX_END >= analysis['count']:\n",
    "        print(f\"‚ö†Ô∏è C·∫£nh b√°o: INDEX_END ({INDEX_END}) v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng m·∫´u ({analysis['count']}).\")\n",
    "        print(f\"S·∫Ω t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh INDEX_END v·ªÅ {analysis['count'] - 1}.\")\n",
    "        INDEX_END = analysis['count'] - 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ S·∫µn s√†ng ch·∫°y quy tr√¨nh cho c√°c m·∫´u t·ª´ {INDEX_START} ƒë·∫øn {INDEX_END}.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚õî Dataset ch∆∞a ƒë∆∞·ª£c t·∫£i, kh√¥ng th·ªÉ ti·∫øp t·ª•c.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainFlow(source_text: str, max_iters: int, min_improve: float) -> dict:\n",
    "    \n",
    "    best_reasoning_json = None   # store full JSON string\n",
    "    best_score = 0.0\n",
    "    history_log = {\n",
    "        \"source_text\": source_text,\n",
    "        \"iterations\": []\n",
    "    }\n",
    "    current_feedback = None\n",
    "    last_reasoning_json = \"\"\n",
    "\n",
    "    for step in range(1, max_iters + 1):\n",
    "            \n",
    "        print(f\"\\nüîÑ V√≤ng {step} ...\")\n",
    "\n",
    "        # Run Reason + (optional) Refine step, passing full previous JSON\n",
    "        reasoning_json = flow_reason.run(\n",
    "            client=REASON_CLIENT,\n",
    "            reason_prompt=FIRST_REASON,\n",
    "            refine_prompt=REFINE_REASON,\n",
    "            generation_params=REASON_PARAMS,\n",
    "            source_text=source_text,\n",
    "            current_reasoning=last_reasoning_json,\n",
    "            feedback=current_feedback,\n",
    "        )\n",
    "        \n",
    "        if not reasoning_json or not str(reasoning_json).strip():\n",
    "            print(\"‚õî L·ªói: Reasoning tr·∫£ v·ªÅ r·ªóng, d·ª´ng v√≤ng l·∫∑p.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nüîÑ Reaoning Result:\\n{reasoning_json}\")\n",
    "\n",
    "        last_reasoning_json = reasoning_json\n",
    "\n",
    "        critical_output = {}\n",
    "        critical_output = flow_critic.run(\n",
    "            client=CRITIC_CLIENT,\n",
    "            critic_prompt=FIRST_CRITIC,\n",
    "            refine_prompt=REFINE_CRITIC,\n",
    "            generation_params=CRITIC_PARAMS,\n",
    "            source_text=source_text,\n",
    "            reasoning_output=reasoning_json,\n",
    "            prev_result=critical_output,\n",
    "        )\n",
    "        \n",
    "        if \"error\" in critical_output:\n",
    "            print(f\"‚õî L·ªói t·ª´ Critical: {critical_output['error']}\")\n",
    "            if \"raw_response\" in critical_output:\n",
    "                print(\"--- RAW OUTPUT ---\")\n",
    "                print(critical_output[\"raw_response\"])\n",
    "                print(\"------------------\")\n",
    "            history_log[\"iterations\"].append({\"round\": step, \"error\": critical_output})\n",
    "            break\n",
    "        \n",
    "        average_score = helpers.average_score(critical_output)\n",
    "        current_feedback = critical_output.get(\"feedback_text\", \"\")\n",
    "\n",
    "        print(f\"üìä ƒêi·ªÉm TBC: {average_score:.2f}\")\n",
    "        print(f\"üìù Nh·∫≠n x√©t (To√†n b·ªô JSON): {json.dumps(critical_output, ensure_ascii=False, indent=2)}\\n\")\n",
    "\n",
    "        history_log[\"iterations\"].append({\n",
    "            \"round\": step, \n",
    "            \"article:\": source_text,\n",
    "            \"reasoning\": reasoning_json,\n",
    "            \"evaluation\": critical_output.get(\"scoring\", {}),\n",
    "            \"average_score\": average_score,\n",
    "            \"feedback\": current_feedback\n",
    "        })\n",
    "\n",
    "        # early exit rules\n",
    "        if average_score > 4.8:\n",
    "            best_reasoning_json, best_score = reasoning_json, average_score\n",
    "            print(\"‚úÖ K·∫øt qu·∫£ t·ªët, d·ª´ng s·ªõm\")\n",
    "            break\n",
    "\n",
    "\n",
    "        if best_score == 0:  # first run\n",
    "            best_reasoning_json, best_score = reasoning_json, average_score\n",
    "            print(f\"üìà L·∫ßn ƒë·∫ßu, ƒëi·ªÉm: {best_score:.2f}\")\n",
    "        elif (average_score - best_score) > min_improve:\n",
    "            best_reasoning_json, best_score = reasoning_json, average_score\n",
    "            print(f\"üìà C·∫£i thi·ªán t·ªët, ƒëi·ªÉm m·ªõi: {best_score:.2f}\")\n",
    "        elif (average_score - best_score) >= 0:\n",
    "            best_reasoning_json, best_score = reasoning_json, average_score\n",
    "            print(f\"‚õî Kh√¥ng c·∫£i thi·ªán ƒë√°ng k·ªÉ, ƒëi·ªÉm m·ªõi: {best_score:.2f}\")\n",
    "        else:\n",
    "            best_reasoning_json, best_score = reasoning_json, average_score\n",
    "            print(f\"‚õî K·∫øt qu·∫£ gi·∫£m s√∫t, ƒëi·ªÉm m·ªõi: {best_score:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"best_reasoning\": best_reasoning_json,\n",
    "        \"best_score\": best_score,\n",
    "        \"history\": history_log\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ PATCH CELL: CH·∫†Y BATCH SAFE-SKIP\n",
    "successful_runs = 0\n",
    "\n",
    "print(f\"üöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y H√ÄNG LO·∫†T CHO {INDEX_END - INDEX_START + 1} M·∫™U... ({INDEX_START} ‚Üí {INDEX_END})\")\n",
    "print(f\"K·∫øt qu·∫£ ‚Üí {BATCH_HISTORY_FILE.resolve()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(INDEX_START, INDEX_END + 1):\n",
    "    print(f\"\\n\\n--- üîÑ M·∫´u #{i} ---\")\n",
    "\n",
    "    raw_text = ds_proc.get_content_by_index_internal(HF_DATASET, i)\n",
    "    if not raw_text:\n",
    "        print(f\"‚õî Kh√¥ng l·∫•y ƒë∆∞·ª£c article cho index {i}\")\n",
    "        continue\n",
    "\n",
    "    # --- TI·ªÄN X·ª¨ L√ù CHU·∫®N ---\n",
    "    text = raw_text.replace(\"\\n\", \" \")        # b·ªè xu·ªëng d√≤ng\n",
    "    text = \" \".join(text.split())             # normalize space\n",
    "\n",
    "    input_article = text\n",
    "\n",
    "    try:\n",
    "        print(f\"B·∫Øt ƒë·∫ßu mainFlow ‚Üí {FLOW_PARAMS}\")\n",
    "        result = mainFlow(\n",
    "            input_article,\n",
    "            max_iters=FLOW_PARAMS.get(\"max_iters\", 3),\n",
    "            min_improve=FLOW_PARAMS.get(\"min_improve\", 0.05)\n",
    "        )\n",
    "\n",
    "        history_key = f\"index_{i}\"\n",
    "        history_data = result[\"history\"].get(\"iterations\", [])\n",
    "\n",
    "        UTL.update_json_dict(history_key, history_data, BATCH_HISTORY_FILE, indent=2)\n",
    "        print(f\"‚úÖ L∆∞u xong {history_key}\")\n",
    "        successful_runs += 1\n",
    "\n",
    "        print(\"\\n\" + \".\"*50)\n",
    "        print(f\"üéØ ƒêi·ªÉm: {result['best_score']}\")\n",
    "        print(f\"üß† T√≥m t·∫Øt: {result['best_reasoning']}\")\n",
    "        print(\".\"*50)\n",
    "\n",
    "    except PipelineAbortSample as e:\n",
    "        print(f\"‚ö†Ô∏è Skip m·∫´u {i}: {e}\")\n",
    "        UTL.update_json_dict(f\"index_{i}\", {\"status\":\"skipped\",\"error\":str(e)}, BATCH_HISTORY_FILE, indent=2)\n",
    "        continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fatal l·ªói t·∫°i index {i}: {e}\")\n",
    "        UTL.update_json_dict(f\"index_{i}\", {\"status\":\"fatal\",\"error\":str(e)}, BATCH_HISTORY_FILE, indent=2)\n",
    "        raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ HO√ÄN T·∫§T\")\n",
    "print(f\"üéâ Th√†nh c√¥ng: {successful_runs} m·∫´u\")\n",
    "print(f\"üì¶ Log: {BATCH_HISTORY_FILE.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bruh-minimal-imports",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
