# Libraries/Client_Llama.py

import json
import time
import requests
from typing import Optional, Dict, Any, List

# ==============================

class LocalLlamaClient:
    def __init__(self, 
                 host: str = "http://localhost:8080", 
                 timeout: int = 270,
                 retry: int = 3, 
                 wait_timeout: int = 300):
        
        self.host = host.rstrip("/")
        self.timeout = timeout 
        self.health_timeout = 5
        self.retry = retry

        # ch·ªçn endpoint m·∫∑c ƒë·ªãnh, c√≥ th·ªÉ c·∫≠p nh·∫≠t sau health-check
        self._completion_endpoint = "/completion"    # llama.cpp server
        self._alt_endpoints = ["/v1/completions", "/v1/chat/completions"]

        self.wait_for_server_ready(wait_timeout)
        
    def wait_for_server_ready(self, wait_timeout: int):
        """
        H·ªèi thƒÉm (poll) endpoint /health cho ƒë·∫øn khi server "ready"
        ho·∫∑c h·∫øt th·ªùi gian ch·ªù (wait_timeout).
        Ch·∫•p nh·∫≠n {"status":"ok"} ho·∫∑c {"ready":true} ho·∫∑c HTTP 200 v·ªõi text "ok".
        """
        start_time = time.time()
        url = f"{self.host}/health"
        
        print(f"‚è≥ ƒêang ki·ªÉm tra Llama server t·∫°i {self.host}...")
        
        while True:
            elapsed = time.time() - start_time
            if elapsed > wait_timeout:
                print(f"‚ùå Server kh√¥ng s·∫µn s√†ng sau {wait_timeout} gi√¢y.")
                raise TimeoutError(f"Server t·∫°i {self.host} kh√¥ng s·∫µn s√†ng sau {wait_timeout}s.")

            try:
                res = requests.get(url, timeout=self.health_timeout)
                ok = False
                try:
                    data = res.json()
                    status = str(data.get("status", "")).lower()
                    ready = bool(data.get("ready", False))
                    if status == "ok" or ready:
                        ok = True
                except json.JSONDecodeError:
                    if res.status_code == 200 and "ok" in res.text.lower():
                        ok = True

                if ok:
                    print(f"‚úÖ Server ƒë√£ s·∫µn s√†ng.")
                    break
                else:
                    print(f"‚è≥ Server ch∆∞a s·∫µn s√†ng (HTTP {res.status_code}). Th·ª≠ l·∫°i sau 5s...")

            except requests.exceptions.ConnectionError:
                print(f"‚è≥ ƒêang ch·ªù k·∫øt n·ªëi ƒë·∫øn server t·∫°i {self.host}... Th·ª≠ l·∫°i sau 5s...")
            
            except requests.exceptions.RequestException as e:
                print(f"‚ö†Ô∏è L·ªói health check: {e}. Th·ª≠ l·∫°i sau 5s...")

            time.sleep(5)

    def _post(self, endpoint: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """G·ª≠i y√™u c·∫ßu POST v√† x·ª≠ l√Ω retry + fallback endpoint."""
        tried = [endpoint] + [ep for ep in self._alt_endpoints if ep != endpoint]

        for ep in tried:
            url = f"{self.host}{ep}"
            for attempt in range(self.retry):
                try:
                    res = requests.post(url, json=payload, timeout=self.timeout) 

                    if res.status_code == 200:
                        try:
                            return res.json()
                        except json.JSONDecodeError:
                            return {"error": f"INVALID_JSON_RESPONSE at {ep}"}

                    print(f"[LLAMA API WARNING] HTTP {res.status_code} at {ep}: {res.text}")
                    # N·∫øu 404, th·ª≠ endpoint kh√°c ngay
                    if res.status_code == 404:
                        break
                    time.sleep(1)

                except requests.exceptions.RequestException as e:
                    print(f"[LLAMA API ERROR] {str(e)} (endpoint {ep})")
                    time.sleep(1)

        return {"error": "LLAMA_REQUEST_FAILED"}

    def reset(self):
        """Reset model session / KV cache tr√™n llama.cpp (n·∫øu h·ªó tr·ª£)."""
        url = f"{self.host}/reset"
        try:
            res = requests.post(url, timeout=10)
            if res.status_code == 200:
                print("üßπ Phi√™n ƒë√£ reset (KV cache cleared)")
            else:
                print(f"‚ö†Ô∏è Reset l·ªói: HTTP {res.status_code} ‚Üí {res.text}")
        except Exception as e:
            print(f"‚ùå Kh√¥ng reset ƒë∆∞·ª£c Llama server: {e}")

    def __call__(self,
                 prompt: str,
                 max_tokens: int = 512,
                 temperature: float = 0.7,
                 top_p: float = 0.9,
                 stop: Optional[List[str]] = None,
                 grammar: Optional[str] = None,
                 json_mode: bool = False) -> Dict[str, Any]:
        
        payload = {
            "prompt": prompt,
            "stream": False,
            "n_predict": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
        }

        if stop:
            payload["stop"] = stop
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        elif grammar:
            payload["grammar"] = grammar
            
        data = self._post(self._completion_endpoint, payload)

        if "error" in data:
            return {"choices": [{"text": f"[LLAMA_ERROR] {data['error']}"}]}
        
        # Chu·∫©n h√≥a theo schema llama.cpp server
        content = data.get("content", "")
        if not content and isinstance(data.get("choices"), list):
            # openai-like
            try:
                content = data["choices"][0]["message"]["content"]
            except Exception:
                try:
                    content = data["choices"][0].get("text","")
                except Exception:
                    content = ""

        return {
            "choices": [
                {"text": content}
            ]
        }
