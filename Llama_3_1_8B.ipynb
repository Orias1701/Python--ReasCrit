{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import regex\n",
        "import random\n",
        "from pathlib import Path\n",
        "from statistics import mean\n",
        "from huggingface_hub import InferenceClient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG_PATH = Path(\"Config/huggingface.json\")\n",
        "PROMPT_REASON_PATH = Path(\"Prompt/Reasoning.txt\")\n",
        "PROMPT_CRITIC_PATH = Path(\"Prompt/Critical.txt\")\n",
        "TESTDATA_PATH = Path(\"Data/TestData.json\")\n",
        "\n",
        "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "HF_TOKEN = config.get(\"HF_TOKEN\")\n",
        "REASON_MODEL = config.get(\"REASON_MODEL\")\n",
        "CRITICAL_MODEL = config.get(\"CRITICAL_MODEL\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "reason_client = InferenceClient(api_key=HF_TOKEN)\n",
        "critic_client = InferenceClient(api_key=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_REASON_BASE = \"\"\"\n",
        "    B·∫°n l√† h·ªá th·ªëng t√≥m t·∫Øt vƒÉn b·∫£n th√¥ng minh.\n",
        "    H√£y t·∫°o b·∫£n t√≥m t·∫Øt ch√≠nh x√°c, ng·∫Øn g·ªçn, m·∫°ch l·∫°c v√† c√≥ th·ªÉ gi·∫£i th√≠ch.\n",
        "    1. Ph√¢n t√≠ch n·ªôi dung ch√≠nh, th·ª±c th·ªÉ, h√†nh ƒë·ªông.\n",
        "    2. L·ªçc √Ω quan tr·ªçng.\n",
        "    3. Vi·∫øt t√≥m t·∫Øt ‚â§ 100 t·ª´.\n",
        "\n",
        "    ƒê·∫ßu ra:\n",
        "    [Reasoning trace]: Ch·ªß ƒë·ªÅ, C√°c √Ω ch√≠nh, C√°c √Ω lo·∫°i b·ªè.\n",
        "    [VƒÉn b·∫£n t√≥m t·∫Øt]: ƒëo·∫°n t√≥m t·∫Øt cu·ªëi c√πng.\n",
        "\n",
        "    VƒÉn b·∫£n g·ªëc:\n",
        "    {input_text}\n",
        "    \"\"\"\n",
        "\n",
        "PROMPT_CRITIC_BASE = \"\"\"\n",
        "    B·∫°n l√† chuy√™n gia ƒë√°nh gi√° reasoning.\n",
        "    H√£y ƒë·ªçc k·ªπ vƒÉn b·∫£n g·ªëc v√† reasoning, sau ƒë√≥ ƒë√°nh gi√° theo 6 ti√™u ch√≠.\n",
        "    Tr·∫£ v·ªÅ JSON:\n",
        "    {\n",
        "    \"factuality\": <1‚Äì5>,\n",
        "    \"clarity\": <1‚Äì5>,\n",
        "    \"logical_coherence\": <1‚Äì5>,\n",
        "    \"coverage\": <1‚Äì5>,\n",
        "    \"utility\": <1‚Äì5>,\n",
        "    \"consistency\": <1‚Äì5>,\n",
        "    \"nhan_xet_chung\": \"...\",\n",
        "    \"goi_y_chinh_sua\": \"...\"\n",
        "    }\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_prompt(path: Path, default_text: str) -> str:\n",
        "    if path.exists():\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read().strip()\n",
        "    else:\n",
        "        print(\"DEFAUT !!\")\n",
        "        return default_text\n",
        "    \n",
        "REASON_PROMPT = load_prompt(\n",
        "    PROMPT_REASON_PATH,\n",
        "    PROMPT_REASON_BASE,\n",
        ")\n",
        "\n",
        "CRITIC_PROMPT = load_prompt(\n",
        "    PROMPT_CRITIC_PATH,\n",
        "    PROMPT_CRITIC_BASE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HELPERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jsonParse(text: str):\n",
        "    text = text.strip()\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        match = regex.search(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=regex.DOTALL)\n",
        "        if match:\n",
        "            json_str = match.group(0)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "        raise ValueError(f\"‚ùå Kh√¥ng parse ƒë∆∞·ª£c JSON t·ª´ ph·∫£n h·ªìi:\\n{text[:400]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def randomContent() -> str:\n",
        "    if not TESTDATA_PATH.exists():\n",
        "        raise FileNotFoundError(\"‚ùå Thi·∫øu file Data/TestData.json\")\n",
        "    with open(TESTDATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    items = [x for x in data if \"content\" in x]\n",
        "    if not items:\n",
        "        raise ValueError(\"‚ùå Kh√¥ng t√¨m th·∫•y tr∆∞·ªùng 'content' trong TestData.json\")\n",
        "    sample = random.choice(items)\n",
        "    print(f\"üìò Ch·ªçn ng·∫´u nhi√™n m·ª•c: {sample.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}\")\n",
        "    return sample[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def averageScore(critical_output: dict) -> float:\n",
        "    keys = [\"factuality\", \"clarity\", \"logical_coherence\", \"coverage\", \"utility\", \"consistency\"]\n",
        "    vals = [critical_output[k] for k in keys if isinstance(critical_output.get(k), (int, float))]\n",
        "    return mean(vals) if vals else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### REASONING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reasoningRun(text: str) -> str:\n",
        "    prompt = REASON_PROMPT.format(input_text=text)\n",
        "    completion = reason_client.text_generation(\n",
        "        model=REASON_MODEL,\n",
        "        prompt=prompt,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9,\n",
        "        details=True,\n",
        "    )\n",
        "\n",
        "    if isinstance(completion, dict) and \"generated_text\" in completion:\n",
        "        return completion[\"generated_text\"].strip()\n",
        "    elif isinstance(completion, list):\n",
        "        return \"\\n\".join([c.get(\"generated_text\", \"\") for c in completion]).strip()\n",
        "    else:\n",
        "        return str(completion).strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CRITICAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def criticalRun(source_text: str, reasoning_output: str) -> dict:\n",
        "    prompt = (\n",
        "        f\"{CRITIC_PROMPT}\\n\\n\"\n",
        "        f\"[VƒÉn b·∫£n g·ªëc]\\n{source_text}\\n\\n\"\n",
        "        f\"[Reasoning + Summary]\\n{reasoning_output}\\n\\n\"\n",
        "        \"Ch·ªâ tr·∫£ JSON duy nh·∫•t.\"\n",
        "    )\n",
        "\n",
        "    completion = critic_client.text_generation(\n",
        "        model=CRITICAL_MODEL,\n",
        "        prompt=prompt,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        details=True,\n",
        "    )\n",
        "\n",
        "    # L·∫•y to√†n b·ªô output t·ª´ m√¥ h√¨nh\n",
        "    if isinstance(completion, dict) and \"generated_text\" in completion:\n",
        "        raw_output = completion[\"generated_text\"]\n",
        "    elif isinstance(completion, list):\n",
        "        raw_output = \"\\n\".join([c.get(\"generated_text\", \"\") for c in completion])\n",
        "    else:\n",
        "        raw_output = str(completion)\n",
        "\n",
        "    return jsonParse(raw_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MAIN FLOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainFlow(source_text: str, max_iters=3, min_improve=0.05):\n",
        "    best_reason, best_score = None, 0\n",
        "    history = []\n",
        "\n",
        "    for step in range(1, max_iters + 1):\n",
        "        print(f\"\\nüîÑ V√≤ng {step} ...\")\n",
        "        reasoning_output = reasoningRun(source_text)\n",
        "        critical_output = criticalRun(source_text, reasoning_output)\n",
        "        average_score = averageScore(critical_output)\n",
        "\n",
        "        print(f\"üìä ƒêi·ªÉm TBC: {average_score:.2f}\")\n",
        "        print(f\"üìù Nh·∫≠n x√©t: {critical_output}\\n\")\n",
        "\n",
        "        history.append({\"round\": step, \"score\": average_score, \"reasoning\": reasoning_output, \"evaluation\": critical_output})\n",
        "        if average_score > best_score + min_improve:\n",
        "            best_reason, best_score = reasoning_output, average_score\n",
        "        else:\n",
        "            print(\"‚õî Kh√¥ng c·∫£i thi·ªán ƒë√°ng k·ªÉ, d·ª´ng.\")\n",
        "            break\n",
        "\n",
        "    return {\"best_reasoning\": best_reason, \"best_score\": best_score, \"history\": history}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    inputPara = randomContent()\n",
        "    result = mainFlow(inputPara)\n",
        "\n",
        "    print(\"\\nüéØ K·∫øt qu·∫£ cu·ªëi c√πng:\")\n",
        "    print(json.dumps({\n",
        "        \"best_score\": result[\"best_score\"],\n",
        "        \"best_reasoning\": result[\"best_reasoning\"][:400] if result[\"best_reasoning\"] else None\n",
        "    }, ensure_ascii=False, indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "master",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
